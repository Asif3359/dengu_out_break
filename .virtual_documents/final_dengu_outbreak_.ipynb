import pandas as pd  # pyright: ignore[reportMissingImports]
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns  # pyright: ignore[reportMissingModuleSource]
import plotly.express as px  # pyright: ignore[reportMissingImports]
import plotly.graph_objects as go  # pyright: ignore[reportMissingImports]
import plotly.io as pio  # pyright: ignore[reportMissingImports]
from scipy import stats  # pyright: ignore[reportMissingImports]

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")



df = pd.read_csv("final_data.csv")
df.info()





# ============================================================================
# SETUP: Check for Existing Models & Prepare Data
# ============================================================================

import os
import pickle
import xgboost as xgb
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Set random seeds
np.random.seed(42)
tf.random.set_seed(42)

print("=" * 70)
print("MODEL TRAINING SETUP")
print("=" * 70)

# Check for existing models
print("\nChecking for existing pretrained models...")
existing_models = {
    'LSTM (dengue_outbreak_model.h5)': os.path.exists('dengue_outbreak_model.h5'),
    'LSTM (best_dengue_model.h5)': os.path.exists('best_dengue_model.h5'),
    'XGBoost': os.path.exists('xgb_dengue_model.pkl'),
    'Scalers': os.path.exists('feature_scaler.pkl') and os.path.exists('target_scaler.pkl')
}

for model_name, exists in existing_models.items():
    status = "âœ“ Found" if exists else "âœ— Not found"
    print(f"  {status}: {model_name}")

# Load and prepare data
print("\n" + "=" * 70)
print("DATA PREPARATION")
print("=" * 70)

df['calendar_start_date'] = pd.to_datetime(df['calendar_start_date'])
df = df.sort_values(['ISO_A0', 'calendar_start_date']).reset_index(drop=True)

print(f"\nâœ“ Loaded {len(df):,} records")
print(f"  Date range: {df['calendar_start_date'].min()} to {df['calendar_start_date'].max()}")
print(f"  Countries: {df['ISO_A0'].nunique()}")

# Configuration
FORECAST_HORIZON = 7  # Predict next 7 days
SEQUENCE_LENGTH = 30  # Look back 30 days for LSTM
TEST_SIZE = 0.2
VALIDATION_SIZE = 0.1

print(f"\nConfiguration:")
print(f"  Forecast Horizon: {FORECAST_HORIZON} days")
print(f"  Sequence Length (LSTM): {SEQUENCE_LENGTH} days")
print(f"  Test Size: {TEST_SIZE*100}%")
print(f"  Validation Size: {VALIDATION_SIZE*100}%")



# ============================================================================
# PREPARE FEATURES
# ============================================================================

# Select features (exclude identifiers and target)
feature_cols = [col for col in df.columns 
                if col not in ['ISO_A0', 'calendar_start_date', 'dengue_total', 'region']]

# Get numeric features only
numeric_features = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()

print(f"Selected {len(numeric_features)} numeric features for modeling")

# Clean data: Handle infinity and extreme values
print("\nCleaning data...")
for col in numeric_features:
    df[col] = df[col].replace([np.inf, -np.inf], np.nan)
    median_val = df[col].median()
    if pd.isna(median_val):
        median_val = 0.0
    df[col] = df[col].fillna(median_val)
    # Clip extreme values
    upper_bound = df[col].quantile(0.999)
    lower_bound = df[col].quantile(0.001)
    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

print("âœ“ Data cleaned")

# Create location identifier
df['location_id'] = df['ISO_A0'].astype(str)






# ============================================================================
# XGBOOST MODEL: Prepare Data & Train
# ============================================================================

print("=" * 70)
print("XGBOOST MODEL TRAINING")
print("=" * 70)

# For XGBoost, we'll predict each day separately (multi-output regression)
# Create features: use rolling window features + current features
print("\nPreparing XGBoost data...")

# Create sequences for XGBoost (flatten time series into features)
def create_xgb_features(df, feature_cols, seq_length, forecast_horizon):
    """Create features for XGBoost by flattening time series"""
    X_list = []
    y_list = []
    
    for location_id in df['location_id'].unique():
        location_data = df[df['location_id'] == location_id].copy()
        location_data = location_data.sort_values('calendar_start_date').reset_index(drop=True)
        
        if len(location_data) < seq_length + forecast_horizon:
            continue
        
        # Extract features and target
        features = location_data[feature_cols].values
        target = location_data['dengue_total'].values
        
        # Create sequences
        for i in range(len(location_data) - seq_length - forecast_horizon + 1):
            # Flatten sequence into single row
            X_flat = features[i:i+seq_length].flatten()
            # Target: next 7 days
            y_flat = target[i+seq_length:i+seq_length+forecast_horizon]
            
            X_list.append(X_flat)
            y_list.append(y_flat)
    
    return np.array(X_list), np.array(y_list)

# Create XGBoost features
print("Creating sequences...")
X_xgb, y_xgb = create_xgb_features(df, numeric_features, SEQUENCE_LENGTH, FORECAST_HORIZON)

print(f"âœ“ Created {len(X_xgb)} samples")
print(f"  X shape: {X_xgb.shape}")
print(f"  y shape: {y_xgb.shape}")

# Clean data
X_xgb = np.nan_to_num(X_xgb, nan=0.0, posinf=0.0, neginf=0.0)
y_xgb = np.nan_to_num(y_xgb, nan=0.0, posinf=0.0, neginf=0.0)
X_xgb = np.clip(X_xgb, -1e10, 1e10)
y_xgb = np.clip(y_xgb, -1e10, 1e10)

# Split data
split_idx = int(len(X_xgb) * (1 - TEST_SIZE))
X_xgb_train, X_xgb_test = X_xgb[:split_idx], X_xgb[split_idx:]
y_xgb_train, y_xgb_test = y_xgb[:split_idx], y_xgb[split_idx:]

print(f"\nData split:")
print(f"  Train: {len(X_xgb_train):,} samples")
print(f"  Test:  {len(X_xgb_test):,} samples")



# Train XGBoost models (one for each forecast day)
print("\nTraining XGBoost models (one per forecast day)...")

xgb_models = []
xgb_scaler = MinMaxScaler()

# Scale features
X_xgb_train_scaled = xgb_scaler.fit_transform(X_xgb_train)
X_xgb_test_scaled = xgb_scaler.transform(X_xgb_test)

for day in range(FORECAST_HORIZON):
    print(f"  Training model for Day {day + 1}...")
    
    # Train model for this day
    # Note: In XGBoost 3.x, early_stopping_rounds goes in constructor
    model = xgb.XGBRegressor(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        verbosity=0,
        early_stopping_rounds=20  # Moved to constructor in XGBoost 3.x
    )
    
    # Fit with eval_set (but early_stopping_rounds is in constructor)
    model.fit(
        X_xgb_train_scaled,
        y_xgb_train[:, day],
        eval_set=[(X_xgb_test_scaled, y_xgb_test[:, day])],
        verbose=False
    )
    
    xgb_models.append(model)
    print(f"    âœ“ Day {day + 1} model trained")

# Save XGBoost models
with open('xgb_dengue_models.pkl', 'wb') as f:
    pickle.dump(xgb_models, f)
with open('xgb_scaler.pkl', 'wb') as f:
    pickle.dump(xgb_scaler, f)

print("\nâœ“ XGBoost models saved")

# Evaluate XGBoost
print("\n" + "=" * 70)
print("XGBOOST EVALUATION")
print("=" * 70)

y_xgb_pred = np.zeros_like(y_xgb_test)
for day in range(FORECAST_HORIZON):
    y_xgb_pred[:, day] = xgb_models[day].predict(X_xgb_test_scaled)

# Ensure non-negative
y_xgb_pred = np.maximum(y_xgb_pred, 0)

xgb_metrics = []
for day in range(FORECAST_HORIZON):
    y_true = y_xgb_test[:, day]
    y_pred = y_xgb_pred[:, day]
    
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    mask = y_true > 0
    if mask.sum() > 0:
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / (y_true[mask] + 1))) * 100
    else:
        mape = np.nan
    
    xgb_metrics.append({
        'Day': day + 1,
        'RMSE': rmse,
        'MAE': mae,
        'MAPE': mape,
        'RÂ²': r2
    })
    
    print(f"\nDay {day + 1}:")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  MAE:  {mae:.2f}")
    print(f"  MAPE: {mape:.2f}%")
    print(f"  RÂ²:   {r2:.4f}")

xgb_metrics_df = pd.DataFrame(xgb_metrics)
print("\n" + "-" * 70)
print("XGBoost Average Metrics:")
print(f"  RMSE: {xgb_metrics_df['RMSE'].mean():.2f}")
print(f"  MAE:  {xgb_metrics_df['MAE'].mean():.2f}")
print(f"  MAPE: {xgb_metrics_df['MAPE'].mean():.2f}%")
print(f"  RÂ²:   {xgb_metrics_df['RÂ²'].mean():.4f}")






# ============================================================================
# LSTM MODEL: Check for Existing Model or Train New
# ============================================================================

print("=" * 70)
print("LSTM MODEL")
print("=" * 70)

# Check if pretrained model exists
use_pretrained = os.path.exists('dengue_outbreak_model.h5') and \
                 os.path.exists('feature_scaler.pkl') and \
                 os.path.exists('target_scaler.pkl') and \
                 os.path.exists('feature_columns.pkl')

if use_pretrained:
    print("\nâœ“ Found pretrained LSTM model! Loading...")
    
    try:
        # Load pretrained model
        lstm_model = keras.models.load_model('dengue_outbreak_model.h5')
        
        # Load scalers
        with open('feature_scaler.pkl', 'rb') as f:
            feature_scaler = pickle.load(f)
        with open('target_scaler.pkl', 'rb') as f:
            target_scaler = pickle.load(f)
        with open('feature_columns.pkl', 'rb') as f:
            saved_features = pickle.load(f)
        
        print("âœ“ Pretrained model loaded successfully!")
        print(f"  Model: dengue_outbreak_model.h5")
        print(f"  Features: {len(saved_features)}")
        
        # Use saved features if they match
        if set(saved_features) == set(numeric_features):
            print("  âœ“ Feature columns match")
            numeric_features_lstm = saved_features
        else:
            print("  âš  Feature columns differ, using current features")
            numeric_features_lstm = numeric_features
        
    except Exception as e:
        print(f"âš  Error loading pretrained model: {e}")
        print("  Will train new model instead...")
        use_pretrained = False

if not use_pretrained:
    print("\nTraining new LSTM model...")
    
    # Create LSTM sequences
    def create_lstm_sequences(data, target_col, feature_cols, seq_length, forecast_horizon, location_ids):
        X_sequences = []
        y_sequences = []
        
        for location_id in location_ids.unique():
            location_data = data[data['location_id'] == location_id].copy()
            location_data = location_data.sort_values('calendar_start_date').reset_index(drop=True)
            
            if len(location_data) < seq_length + forecast_horizon:
                continue
            
            features = location_data[feature_cols].values
            target = location_data[target_col].values
            
            for i in range(len(location_data) - seq_length - forecast_horizon + 1):
                X_sequences.append(features[i:i+seq_length])
                y_sequences.append(target[i+seq_length:i+seq_length+forecast_horizon])
        
        return np.array(X_sequences), np.array(y_sequences)
    
    print("Creating LSTM sequences...")
    X_lstm, y_lstm = create_lstm_sequences(
        df, 'dengue_total', numeric_features, SEQUENCE_LENGTH, FORECAST_HORIZON, df['location_id']
    )
    
    print(f"âœ“ Created {len(X_lstm)} sequences")
    print(f"  X shape: {X_lstm.shape}")
    print(f"  y shape: {y_lstm.shape}")
    
    # Clean data
    X_lstm = np.nan_to_num(X_lstm, nan=0.0, posinf=0.0, neginf=0.0)
    y_lstm = np.nan_to_num(y_lstm, nan=0.0, posinf=0.0, neginf=0.0)
    X_lstm = np.clip(X_lstm, -1e10, 1e10)
    y_lstm = np.clip(y_lstm, -1e10, 1e10)
    
    # Scale features
    print("Scaling features...")
    n_samples, n_timesteps, n_features = X_lstm.shape
    X_reshaped = X_lstm.reshape(-1, n_features)
    feature_scaler = MinMaxScaler()
    X_scaled = feature_scaler.fit_transform(X_reshaped)
    X_scaled = X_scaled.reshape(n_samples, n_timesteps, n_features)
    
    # Scale target
    y_reshaped = y_lstm.reshape(-1, FORECAST_HORIZON)
    target_scaler = MinMaxScaler()
    y_scaled = target_scaler.fit_transform(y_reshaped)
    
    # Split data
    split_idx_1 = int(len(X_scaled) * (1 - TEST_SIZE - VALIDATION_SIZE))
    split_idx_2 = int(len(X_scaled) * (1 - TEST_SIZE))
    
    X_train = X_scaled[:split_idx_1]
    y_train = y_scaled[:split_idx_1]
    X_val = X_scaled[split_idx_1:split_idx_2]
    y_val = y_scaled[split_idx_1:split_idx_2]
    X_test = X_scaled[split_idx_2:]
    y_test = y_scaled[split_idx_2:]
    y_test_original = y_lstm[split_idx_2:]
    
    print(f"\nData split:")
    print(f"  Train: {len(X_train):,} samples")
    print(f"  Val:   {len(X_val):,} samples")
    print(f"  Test:  {len(X_test):,} samples")
    
    # Build LSTM model
    print("\nBuilding LSTM model...")
    def build_lstm_model(input_shape, forecast_horizon):
        model = keras.Sequential([
            layers.LSTM(128, return_sequences=True, input_shape=input_shape),
            layers.Dropout(0.2),
            layers.LSTM(64, return_sequences=False),
            layers.Dropout(0.2),
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(32, activation='relu'),
            layers.Dense(forecast_horizon, activation='linear')
        ])
        return model
    
    input_shape = (SEQUENCE_LENGTH, len(numeric_features))
    lstm_model = build_lstm_model(input_shape, FORECAST_HORIZON)
    
    lstm_model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae', 'mape']
    )
    
    print("Model architecture:")
    lstm_model.summary()
    
    # Train model
    print("\nTraining LSTM model...")
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=1
        ),
        keras.callbacks.ModelCheckpoint(
            'best_dengue_model.h5',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    history = lstm_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=50,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )
    
    # Save model and scalers
    lstm_model.save('dengue_outbreak_model.h5')
    with open('feature_scaler.pkl', 'wb') as f:
        pickle.dump(feature_scaler, f)
    with open('target_scaler.pkl', 'wb') as f:
        pickle.dump(target_scaler, f)
    with open('feature_columns.pkl', 'wb') as f:
        pickle.dump(numeric_features, f)
    
    print("\nâœ“ LSTM model trained and saved")
    
    # Evaluate
    print("\n" + "=" * 70)
    print("LSTM EVALUATION")
    print("=" * 70)
    
    y_pred_scaled = lstm_model.predict(X_test, verbose=0)
    y_pred = target_scaler.inverse_transform(y_pred_scaled)
    y_pred = np.maximum(y_pred, 0)
    
    lstm_metrics = []
    for day in range(FORECAST_HORIZON):
        y_true = y_test_original[:, day]
        y_pred_day = y_pred[:, day]
        
        rmse = np.sqrt(mean_squared_error(y_true, y_pred_day))
        mae = mean_absolute_error(y_true, y_pred_day)
        r2 = r2_score(y_true, y_pred_day)
        
        mask = y_true > 0
        if mask.sum() > 0:
            mape = np.mean(np.abs((y_true[mask] - y_pred_day[mask]) / (y_true[mask] + 1))) * 100
        else:
            mape = np.nan
        
        lstm_metrics.append({
            'Day': day + 1,
            'RMSE': rmse,
            'MAE': mae,
            'MAPE': mape,
            'RÂ²': r2
        })
        
        print(f"\nDay {day + 1}:")
        print(f"  RMSE: {rmse:.2f}")
        print(f"  MAE:  {mae:.2f}")
        print(f"  MAPE: {mape:.2f}%")
        print(f"  RÂ²:   {r2:.4f}")
    
    lstm_metrics_df = pd.DataFrame(lstm_metrics)
    print("\n" + "-" * 70)
    print("LSTM Average Metrics:")
    print(f"  RMSE: {lstm_metrics_df['RMSE'].mean():.2f}")
    print(f"  MAE:  {lstm_metrics_df['MAE'].mean():.2f}")
    print(f"  MAPE: {lstm_metrics_df['MAPE'].mean():.2f}%")
    print(f"  RÂ²:   {lstm_metrics_df['RÂ²'].mean():.4f}")
else:
    print("\nUsing pretrained model. To evaluate, run prediction code below.")






# ============================================================================
# MODEL COMPARISON & PREDICTION FUNCTIONS
# ============================================================================

print("=" * 70)
print("MODEL COMPARISON")
print("=" * 70)

if 'xgb_metrics_df' in locals() and 'lstm_metrics_df' in locals():
    comparison = pd.DataFrame({
        'Model': ['XGBoost', 'LSTM'],
        'Avg RMSE': [xgb_metrics_df['RMSE'].mean(), lstm_metrics_df['RMSE'].mean()],
        'Avg MAE': [xgb_metrics_df['MAE'].mean(), lstm_metrics_df['MAE'].mean()],
        'Avg MAPE': [xgb_metrics_df['MAPE'].mean(), lstm_metrics_df['MAPE'].mean()],
        'Avg RÂ²': [xgb_metrics_df['RÂ²'].mean(), lstm_metrics_df['RÂ²'].mean()]
    })
    
    print("\n" + comparison.to_string(index=False))
    
    # Determine best model
    best_model = 'XGBoost' if xgb_metrics_df['RMSE'].mean() < lstm_metrics_df['RMSE'].mean() else 'LSTM'
    print(f"\nðŸ† Best Model (by RMSE): {best_model}")
elif 'xgb_metrics_df' in locals():
    print("\nâœ“ XGBoost trained and evaluated")
    print("  LSTM: Use pretrained model or train new one")
elif 'lstm_metrics_df' in locals():
    print("\nâœ“ LSTM trained and evaluated")
    print("  XGBoost: Not trained in this session")
else:
    print("\nModels available for prediction (may be pretrained)")

# ============================================================================
# PREDICTION FUNCTIONS
# ============================================================================

def predict_xgboost(location_code, date=None):
    """Predict using XGBoost model"""
    if 'xgb_models' not in locals():
        # Load models
        with open('xgb_dengue_models.pkl', 'rb') as f:
            xgb_models = pickle.load(f)
        with open('xgb_scaler.pkl', 'rb') as f:
            xgb_scaler = pickle.load(f)
    
    # Get latest data
    if date is None:
        location_data = df[df['ISO_A0'] == location_code].tail(SEQUENCE_LENGTH)
    else:
        date = pd.to_datetime(date)
        location_data = df[
            (df['ISO_A0'] == location_code) & 
            (df['calendar_start_date'] <= date)
        ].tail(SEQUENCE_LENGTH)
    
    if len(location_data) < SEQUENCE_LENGTH:
        return None
    
    # Prepare features
    features = location_data[numeric_features].values
    X_flat = features.flatten().reshape(1, -1)
    X_scaled = xgb_scaler.transform(X_flat)
    
    # Predict
    predictions = np.array([model.predict(X_scaled)[0] for model in xgb_models])
    predictions = np.maximum(predictions, 0)
    
    return predictions

def predict_lstm(location_code, date=None):
    """Predict using LSTM model"""
    # Load model if not in memory
    if 'lstm_model' not in locals():
        lstm_model = keras.models.load_model('dengue_outbreak_model.h5')
        with open('feature_scaler.pkl', 'rb') as f:
            feature_scaler = pickle.load(f)
        with open('target_scaler.pkl', 'rb') as f:
            target_scaler = pickle.load(f)
        with open('feature_columns.pkl', 'rb') as f:
            feature_cols = pickle.load(f)
    
    # Get latest data
    if date is None:
        location_data = df[df['ISO_A0'] == location_code].tail(SEQUENCE_LENGTH)
    else:
        date = pd.to_datetime(date)
        location_data = df[
            (df['ISO_A0'] == location_code) & 
            (df['calendar_start_date'] <= date)
        ].tail(SEQUENCE_LENGTH)
    
    if len(location_data) < SEQUENCE_LENGTH:
        return None
    
    # Prepare features
    features = location_data[feature_cols].values
    features_scaled = feature_scaler.transform(features)
    features_scaled = features_scaled.reshape(1, SEQUENCE_LENGTH, len(feature_cols))
    
    # Predict
    y_pred_scaled = lstm_model.predict(features_scaled, verbose=0)
    y_pred = target_scaler.inverse_transform(y_pred_scaled)
    y_pred = np.maximum(y_pred, 0)
    
    return y_pred[0]

# Example predictions
print("\n" + "=" * 70)
print("EXAMPLE PREDICTIONS")
print("=" * 70)

test_country = 'BGD'  # Bangladesh
print(f"\nPredicting next 7 days for {test_country}:")

# XGBoost prediction
if os.path.exists('xgb_dengue_models.pkl'):
    xgb_pred = predict_xgboost(test_country)
    if xgb_pred is not None:
        print("\nXGBoost Predictions:")
        for i, pred in enumerate(xgb_pred, 1):
            print(f"  Day {i}: {pred:.1f} cases")
        print(f"  Total (7 days): {xgb_pred.sum():.1f} cases")

# LSTM prediction
if os.path.exists('dengue_outbreak_model.h5'):
    lstm_pred = predict_lstm(test_country)
    if lstm_pred is not None:
        print("\nLSTM Predictions:")
        for i, pred in enumerate(lstm_pred, 1):
            print(f"  Day {i}: {pred:.1f} cases")
        print(f"  Total (7 days): {lstm_pred.sum():.1f} cases")

print("\n" + "=" * 70)
print("âœ… MODEL TRAINING COMPLETE!")
print("=" * 70)
print("\nSaved files:")
if os.path.exists('xgb_dengue_models.pkl'):
    print("  âœ“ xgb_dengue_models.pkl")
    print("  âœ“ xgb_scaler.pkl")
if os.path.exists('dengue_outbreak_model.h5'):
    print("  âœ“ dengue_outbreak_model.h5")
    print("  âœ“ feature_scaler.pkl")
    print("  âœ“ target_scaler.pkl")
    print("  âœ“ feature_columns.pkl")

